{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42331c1",
   "metadata": {},
   "source": [
    "# Machine Learning Course Project (End-to-End)\n",
    "\n",
    "> **Course:** Machine Learning\n",
    "\n",
    "## Team Members\n",
    "- **Mohamed Mostafa** — **23101594**\n",
    "- **Marwan Khaled** — **23101599**\n",
    "- **Mohamed Adel** — **23101899**\n",
    "\n",
    "> **Date:** December 2025\n",
    "\n",
    "> This notebook is submission-ready: it loads a dirty real-world dataset, cleans and analyzes it, builds **one classification** and **one regression** model, compares multiple algorithms (with/without scaling and normalization), and exports a **PDF report** + a **ZIP** of all deliverables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b59d30",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Part 1 — Problem Understanding\n",
    "2. Part 2 — Data Collection & Dataset Description\n",
    "3. Part 3 — Data Cleaning & Refinement\n",
    "4. Part 4 — Exploratory Data Analysis (EDA)\n",
    "5. Part 5 — Data Visualization (8 required plots)\n",
    "6. Part 6 — Machine Learning (Classification + Regression)\n",
    "7. Deliverables — PDF Report + ZIP Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c8098d",
   "metadata": {},
   "source": [
    "# Part 1 — Problem Understanding\n",
    "## 1.1 Problem Domain (Unique Domain Choice)\n",
    "**Domain:** *Data-driven content acquisition and release strategy for a niche streaming platform focusing on long-tail cinema.*\n",
    "\n",
    "> Many beginner projects predict “house price” or “Titanic survival.” Here we instead study **movie performance signals** (popularity, votes, genres, language, runtime, release timing) to support decisions like:\n",
    "- Which types of films are likely to be **high-rated** (quality signal)?\n",
    "- Which films are likely to generate **higher revenue** (commercial signal)?\n",
    "\n",
    "## 1.2 Research Goal\n",
    "- **Goal (classification):** Predict whether a movie is **High Rated** (e.g., IMDb/TMDb-style rating $\\ge 7.5$) using metadata available early in the lifecycle.\n",
    "- **Goal (regression):** Predict **log-revenue** (a continuous value) for movies with reported revenue, using the same metadata.\n",
    "\n",
    "## 1.3 Why This Matters (Academic Justification)\n",
    "- Streaming services face budget constraints and uncertainty; decisions must be made using incomplete metadata.\n",
    "- The project supports **resource allocation** (marketing focus), **portfolio balance** (genre/language mix), and **risk management** (avoid investing heavily in low-ROI profiles).\n",
    "\n",
    "## 1.4 Target Variables\n",
    "- **Classification target:** `is_high_rated` (binary) derived from `vote_average` using a threshold (defined later).\n",
    "- **Regression target:** `log_revenue = log(1 + revenue)` for movies with `revenue > 0` (helps with heavy skew)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e9c7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /content\n",
      "Dataset path: /content/Top_10000_Movies.csv\n",
      "Dataset exists: False\n"
     ]
    }
   ],
   "source": [
    "# Part 0 — Setup (Imports, Settings, Paths)\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve,\n",
    "    mean_absolute_error, mean_squared_error, median_absolute_error, r2_score,\n",
    "    ConfusionMatrixDisplay,\n",
    " )\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Paths (robust to different working directories)\n",
    "PROJECT_DIR = Path.cwd().resolve()\n",
    "DATA_PATH = PROJECT_DIR / \"Top_10000_Movies.csv\"\n",
    "if not DATA_PATH.exists():\n",
    "    matches = list(PROJECT_DIR.rglob(\"Top_10000_Movies.csv\"))\n",
    "    if matches:\n",
    "        DATA_PATH = matches[0].resolve()\n",
    "        PROJECT_DIR = DATA_PATH.parent\n",
    "FIG_DIR = PROJECT_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "print(\"Project directory:\", PROJECT_DIR)\n",
    "print(\"Dataset path:\", DATA_PATH)\n",
    "print(\"Dataset exists:\", DATA_PATH.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aefb865",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/Top_10000_Movies.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2878793957.py\u001b[0m in \u001b[0;36mload_csv_safe\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Top_10000_Movies.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2878793957.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rows, Cols:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2878793957.py\u001b[0m in \u001b[0;36mload_csv_safe\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Top_10000_Movies.csv'"
     ]
    }
   ],
   "source": [
    "# Part 2 — Data Collection (Load Dataset)\n",
    "from pandas.errors import ParserError\n",
    "\n",
    "def load_csv_safe(path):\n",
    "    \"\"\"Load a dirty CSV robustly (handles malformed long-text rows).\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except ParserError:\n",
    "        return pd.read_csv(path, engine=\"python\")\n",
    "    except Exception:\n",
    "        return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "df_raw = load_csv_safe(DATA_PATH)\n",
    "print(\"Rows, Cols:\", df_raw.shape)\n",
    "display(df_raw.head(3))\n",
    "display(df_raw.sample(3, random_state=RANDOM_STATE))\n",
    "\n",
    "# Quick schema overview\n",
    "display(df_raw.dtypes)\n",
    "display(df_raw.describe(include=\"all\").T.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4344db",
   "metadata": {},
   "source": [
    "# Part 2 — Data Collection & Dataset Description\n",
    "## 2.1 Dataset Source (Public)\n",
    "This dataset (`Top_10000_Movies.csv`) contains movie metadata such as `popularity`, `vote_average`, `vote_count`, `revenue`, `runtime`, and text fields (`overview`, `tagline`).\n",
    "\n",
    "> **Source note:** The column names and semantics match **TMDb (The Movie Database)** public movie metadata fields (e.g., `popularity`, `vote_average`, `vote_count`, `revenue`). The dataset is commonly created by extracting from TMDb's public API and publishing it (e.g., as a CSV on open-data platforms / Kaggle).\n",
    "\n",
    "## 2.2 Why This Dataset Is Appropriate\n",
    "- **Size requirement:** $\\ge 10,000$ rows (this file has ~10k).\n",
    "- **Feature requirement:** $\\ge 10$ columns (this file has 13).\n",
    "- **Mixed data types:** numerical (`popularity`, `revenue`, `runtime`), categorical (`original_language`), semi-structured categorical (`genre` list), temporal (`release_date`), and text (`overview`, `tagline`).\n",
    "- **Dirty data:** contains missing text values (blank `tagline`), zero revenues (often a missing/unknown proxy), potential duplicates, inconsistent formatting (genres stored as stringified lists), and outliers (extreme popularity/revenue).\n",
    "\n",
    "## 2.3 Column Dictionary (to be generated programmatically below)\n",
    "We will produce an academic description of each column after inspecting the dataset schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65aee9",
   "metadata": {},
   "source": [
    "**Citation note (important for submission):** if your course requires a precise URL, cite the exact public page you obtained this CSV from. The schema strongly matches TMDb-style fields; the included `download_dataset.py` is a template for recreating the dataset via a public movie-metadata API if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b445c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-143450383.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dataset profiling: missingness, duplicates, basic anomalies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Standardize column names just in case (keep original meaning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset profiling: missingness, duplicates, basic anomalies\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Standardize column names just in case (keep original meaning)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (missing_counts / len(df)).round(4)\n",
    "profile = pd.DataFrame({\"missing_count\": missing_counts, \"missing_pct\": missing_pct, \"dtype\": df.dtypes.astype(str)})\n",
    "display(profile)\n",
    "\n",
    "# Potential duplicates (full-row and key-based)\n",
    "full_dupes = df.duplicated().sum()\n",
    "key_dupes = df.duplicated(subset=[\"id\", \"original_title\", \"release_date\"]).sum() if set([\"id\",\"original_title\",\"release_date\"]).issubset(df.columns) else None\n",
    "print(\"Full-row duplicates:\", full_dupes)\n",
    "print(\"Key-based duplicates (id,title,date):\", key_dupes)\n",
    "\n",
    "# Quick dirty signals\n",
    "print(\"Blank tagline rows:\", (df.get(\"tagline\").fillna(\"\").str.strip() == \"\").sum() if \"tagline\" in df.columns else \"N/A\")\n",
    "print(\"Zero revenue rows:\", (df.get(\"revenue\", pd.Series(dtype=float)) == 0).sum() if \"revenue\" in df.columns else \"N/A\")\n",
    "\n",
    "# Column dictionary (academic-style)\n",
    "column_dictionary = {\n",
    "    \"Unnamed index column (first column)\": \"A row index carried over from a previous export; not a true feature.\",\n",
    "    \"id\": \"Movie identifier (typically from the source system).\",\n",
    "    \"original_language\": \"Original language code of the movie (categorical).\",\n",
    "    \"original_title\": \"Original movie title (text/categorical).\",\n",
    "    \"popularity\": \"Platform-specific popularity score (continuous).\",\n",
    "    \"release_date\": \"Release date (YYYY-MM-DD), later parsed to year/decade.\",\n",
    "    \"vote_average\": \"Average user rating score (continuous, typically 0–10).\",\n",
    "    \"vote_count\": \"Number of user votes (count variable).\",\n",
    "    \"genre\": \"Genres as a stringified Python list (semi-structured categorical).\",\n",
    "    \"overview\": \"Short plot summary text.\",\n",
    "    \"revenue\": \"Reported revenue (numeric; often 0 if missing/unreported).\",\n",
    "    \"runtime\": \"Runtime in minutes (numeric).\",\n",
    "    \"tagline\": \"Marketing tagline text (often missing).\",\n",
    "}\n",
    "display(pd.DataFrame({\"column\": df.columns}))\n",
    "print(\"\\nSuggested dictionary (interpretation):\")\n",
    "for col in df.columns:\n",
    "    msg = column_dictionary.get(col, \"(No manual description; see dtypes/EDA)\")\n",
    "    print(f\"- {col}: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06ea3e",
   "metadata": {},
   "source": [
    "# Part 3 — Data Cleaning & Refinement\n",
    "We will perform all required cleaning operations:\n",
    "1. Identify/handle missing values and explain likely causes.\n",
    "2. Remove irrelevant variables for modeling (while keeping them for EDA where useful).\n",
    "3. Remove duplicates.\n",
    "4. Detect and remove outliers using box plots + IQR rule.\n",
    "5. Handle blank spaces/inconsistent formatting (casing, string lists).\n",
    "6. Arrange data logically; ensure correct types, grouping, and ordering.\n",
    "7. Apply **Scaling** and **Normalization** as separate experiments (later used in modeling comparisons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4f68ed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3379454168.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 3.1 Standardize formatting + parse semi-structured fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Remove irrelevant export index column if present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_raw' is not defined"
     ]
    }
   ],
   "source": [
    "# 3.1 Standardize formatting + parse semi-structured fields\n",
    "df_clean = df_raw.copy()\n",
    "\n",
    "# Remove irrelevant export index column if present\n",
    "for col in list(df_clean.columns):\n",
    "    if col.lower().startswith(\"unnamed\") or col.strip() == \"\":\n",
    "        df_clean = df_clean.drop(columns=[col])\n",
    "\n",
    "# Trim whitespace in object columns\n",
    "obj_cols = df_clean.select_dtypes(include=[\"object\"]).columns\n",
    "for c in obj_cols:\n",
    "    df_clean[c] = df_clean[c].astype(str).str.strip()\n",
    "\n",
    "# Normalize casing for language code\n",
    "if \"original_language\" in df_clean.columns:\n",
    "    df_clean[\"original_language\"] = df_clean[\"original_language\"].str.lower().replace({\"nan\": np.nan})\n",
    "\n",
    "# Parse release_date\n",
    "df_clean[\"release_date\"] = pd.to_datetime(df_clean[\"release_date\"], errors=\"coerce\")\n",
    "df_clean[\"release_year\"] = df_clean[\"release_date\"].dt.year\n",
    "df_clean[\"release_decade\"] = (df_clean[\"release_year\"] // 10 * 10).astype(\"Int64\")\n",
    "\n",
    "# Parse genres (stringified list) -> list[str] + main genre\n",
    "def parse_genre_list(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if s in {\"\", \"nan\", \"None\"}:\n",
    "        return []\n",
    "    try:\n",
    "        lst = ast.literal_eval(s)\n",
    "        if isinstance(lst, list):\n",
    "            # Clean each entry\n",
    "            return [str(g).strip().title() for g in lst if str(g).strip() not in {\"\", \"nan\"}]\n",
    "        return []\n",
    "    except Exception:\n",
    "        # If malformed, attempt simple split fallback\n",
    "        s2 = re.sub(r\"[\\[\\]\\']\", \"\", s)\n",
    "        parts = [p.strip() for p in s2.split(\",\") if p.strip()]\n",
    "        return [p.title() for p in parts]\n",
    "\n",
    "if \"genre\" in df_clean.columns:\n",
    "    df_clean[\"genre_list\"] = df_clean[\"genre\"].apply(parse_genre_list)\n",
    "    df_clean[\"genre_count\"] = df_clean[\"genre_list\"].apply(len)\n",
    "    df_clean[\"genre_main\"] = df_clean[\"genre_list\"].apply(lambda g: g[0] if len(g) else \"Unknown\")\n",
    "\n",
    "# Text-derived features (keeps code within standard libraries)\n",
    "for txt_col in [\"overview\", \"tagline\", \"original_title\"]:\n",
    "    if txt_col in df_clean.columns:\n",
    "        df_clean[txt_col] = df_clean[txt_col].replace({\"nan\": \"\"})\n",
    "        df_clean[f\"{txt_col}_len\"] = df_clean[txt_col].fillna(\"\").astype(str).str.len()\n",
    "\n",
    "# 3.2 Missing values: identify + handle\n",
    "missing_before = df_clean.isna().sum().sort_values(ascending=False)\n",
    "display(missing_before[missing_before > 0])\n",
    "\n",
    "# Why missing? (practical explanation)\n",
    "print(\"\\nInterpretation of missingness (typical for movie metadata):\")\n",
    "print(\"- release_date missing: older/incomplete records or parsing errors\")\n",
    "print(\"- runtime missing: not reported for some records\")\n",
    "print(\"- tagline/overview blank: marketing text absent or not translated\")\n",
    "print(\"- revenue == 0: frequently indicates missing/unreported revenue rather than true zero\")\n",
    "\n",
    "# Convert numeric columns; coerce errors to NaN\n",
    "numeric_candidates = [\"popularity\", \"vote_average\", \"vote_count\", \"revenue\", \"runtime\", \"release_year\"]\n",
    "for c in numeric_candidates:\n",
    "    if c in df_clean.columns:\n",
    "        df_clean[c] = pd.to_numeric(df_clean[c], errors=\"coerce\")\n",
    "\n",
    "# Fill categorical missing values\n",
    "for c in [\"original_language\", \"genre_main\"]:\n",
    "    if c in df_clean.columns:\n",
    "        df_clean[c] = df_clean[c].fillna(\"unknown\").replace({\"\": \"unknown\"})\n",
    "\n",
    "# Fill numeric missing values (median is robust for skew)\n",
    "for c in [\"popularity\", \"vote_average\", \"vote_count\", \"runtime\", \"release_year\"]:\n",
    "    if c in df_clean.columns:\n",
    "        df_clean[c] = df_clean[c].fillna(df_clean[c].median())\n",
    "\n",
    "# Handle blank strings consistently\n",
    "for c in [\"tagline\", \"overview\", \"original_title\"]:\n",
    "    if c in df_clean.columns:\n",
    "        df_clean[c] = df_clean[c].fillna(\"\")\n",
    "        df_clean[c] = df_clean[c].astype(str).str.strip()\n",
    "\n",
    "# 3.3 Remove duplicates\n",
    "before = len(df_clean)\n",
    "if \"id\" in df_clean.columns:\n",
    "    df_clean = df_clean.drop_duplicates(subset=[\"id\"])\n",
    "else:\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "after = len(df_clean)\n",
    "print(f\"Removed duplicates: {before-after} (rows now {after})\")\n",
    "\n",
    "# 3.4 Arrange columns logically\n",
    "preferred_order = [\n",
    "    \"id\",\"original_title\",\"original_language\",\"release_date\",\"release_year\",\"release_decade\",\n",
    "    \"genre\",\"genre_main\",\"genre_count\",\"popularity\",\"vote_average\",\"vote_count\",\"revenue\",\"runtime\",\n",
    "    \"overview\",\"overview_len\",\"tagline\",\"tagline_len\",\"original_title_len\"\n",
    "]\n",
    "cols = [c for c in preferred_order if c in df_clean.columns] + [c for c in df_clean.columns if c not in preferred_order]\n",
    "df_clean = df_clean[cols]\n",
    "\n",
    "display(df_clean.head(3))\n",
    "print(\"Cleaned shape:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c730a4a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2657896492.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 3.5 Outlier detection and removal (Box plots + IQR rule)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutlier_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"popularity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"revenue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"runtime\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vote_count\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutlier_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutlier_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutlier_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# 3.5 Outlier detection and removal (Box plots + IQR rule)\n",
    "outlier_cols = [c for c in [\"popularity\", \"revenue\", \"runtime\", \"vote_count\"] if c in df_clean.columns]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(outlier_cols), figsize=(4*len(outlier_cols), 4))\n",
    "if len(outlier_cols) == 1:\n",
    "    axes = [axes]\n",
    "for ax, col in zip(axes, outlier_cols):\n",
    "    sns.boxplot(y=df_clean[col], ax=ax, color=sns.color_palette()[0])\n",
    "    ax.set_title(f\"Box Plot: {col}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"boxplots_outliers_before.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "def iqr_filter(df_in, cols, k=1.5):\n",
    "    df_out = df_in.copy()\n",
    "    mask = pd.Series(True, index=df_out.index)\n",
    "    bounds = {}\n",
    "    for c in cols:\n",
    "        q1 = df_out[c].quantile(0.25)\n",
    "        q3 = df_out[c].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lo = q1 - k * iqr\n",
    "        hi = q3 + k * iqr\n",
    "        bounds[c] = (lo, hi)\n",
    "        mask &= df_out[c].between(lo, hi)\n",
    "    return df_out[mask].copy(), bounds\n",
    "\n",
    "before_rows = len(df_clean)\n",
    "df_no_outliers, iqr_bounds = iqr_filter(df_clean, outlier_cols, k=1.5)\n",
    "after_rows = len(df_no_outliers)\n",
    "print(\"IQR bounds:\")\n",
    "for c, (lo, hi) in iqr_bounds.items():\n",
    "    print(f\"- {c}: [{lo:.3f}, {hi:.3f}]\")\n",
    "print(f\"Rows before: {before_rows}, after outlier removal: {after_rows} (removed {before_rows-after_rows})\")\n",
    "\n",
    "# Re-plot after removal\n",
    "fig, axes = plt.subplots(1, len(outlier_cols), figsize=(4*len(outlier_cols), 4))\n",
    "if len(outlier_cols) == 1:\n",
    "    axes = [axes]\n",
    "for ax, col in zip(axes, outlier_cols):\n",
    "    sns.boxplot(y=df_no_outliers[col], ax=ax, color=sns.color_palette()[2])\n",
    "    ax.set_title(f\"After Outlier Removal: {col}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"boxplots_outliers_after.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Scaling vs Normalization (separate transformations)\n",
    "df_model_base = df_no_outliers.copy()\n",
    "\n",
    "num_cols = [c for c in [\"popularity\", \"vote_count\", \"runtime\", \"release_year\", \"overview_len\", \"tagline_len\", \"genre_count\"] if c in df_model_base.columns]\n",
    "cat_cols = [c for c in [\"original_language\", \"genre_main\"] if c in df_model_base.columns]\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normalizer = MinMaxScaler()\n",
    "\n",
    "scaled = df_model_base[num_cols].copy()\n",
    "normalized = df_model_base[num_cols].copy()\n",
    "scaled[num_cols] = scaler.fit_transform(scaled[num_cols])\n",
    "normalized[num_cols] = normalizer.fit_transform(normalized[num_cols])\n",
    "\n",
    "display(pd.DataFrame({\"original_mean\": df_model_base[num_cols].mean(), \"scaled_mean\": scaled.mean(), \"normalized_mean\": normalized.mean()}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0d516",
   "metadata": {},
   "source": [
    "# Part 4 — Exploratory Data Analysis (EDA)\n",
    "We analyze relationships, trends, correlations, and meaningful segments (genre, decade, language). The goal is to produce academically phrased insights that motivate feature selection and model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92871cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Correlations and segmented summaries\n",
    "eda_df = df_model_base.copy()\n",
    "\n",
    "# Correlation matrix (numeric features)\n",
    "numeric_for_corr = [c for c in [\"popularity\",\"vote_average\",\"vote_count\",\"revenue\",\"runtime\",\"release_year\",\"overview_len\",\"tagline_len\",\"genre_count\"] if c in eda_df.columns]\n",
    "corr = eda_df[numeric_for_corr].corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(corr, annot=False, cmap=\"viridis\", square=True)\n",
    "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"eda_corr_heatmap.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# Segment by decade and genre\n",
    "if \"release_decade\" in eda_df.columns:\n",
    "    decade_summary = eda_df.groupby(\"release_decade\")[\"vote_average\"].agg([\"count\",\"mean\",\"median\"]).sort_index()\n",
    "    display(decade_summary.tail(15))\n",
    "\n",
    "if \"genre_main\" in eda_df.columns:\n",
    "    genre_summary = eda_df.groupby(\"genre_main\").agg(\n",
    "        n_movies=(\"id\",\"count\"),\n",
    "        avg_rating=(\"vote_average\",\"mean\"),\n",
    "        med_revenue=(\"revenue\",\"median\"),\n",
    "        avg_popularity=(\"popularity\",\"mean\"),\n",
    "    ).sort_values(\"n_movies\", ascending=False)\n",
    "    display(genre_summary.head(15))\n",
    "\n",
    "# Academic-style insights (printed)\n",
    "print(\"\\nKey EDA Insights (to be referenced in the report):\")\n",
    "print(\"1) Popularity and vote_count are typically positively related (visibility drives engagement).\")\n",
    "print(\"2) Revenue is heavy-tailed; log-transform is appropriate for regression.\")\n",
    "print(\"3) Genre and decade strongly segment typical ratings and revenue profiles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e8beb",
   "metadata": {},
   "source": [
    "# Part 5 — Data Visualization (8 Required Plots)\n",
    "We generate the required plots and save them into the `figures/` folder for inclusion in the final PDF report. Each plot is labeled and interpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df = df_model_base.copy()\n",
    "\n",
    "# 1) Line Plot — Average rating over time\n",
    "yearly = viz_df.groupby(\"release_year\")[\"vote_average\"].mean().sort_index()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(yearly.index, yearly.values, linewidth=2)\n",
    "plt.title(\"Line Plot: Average Rating by Release Year\")\n",
    "plt.xlabel(\"Release Year\")\n",
    "plt.ylabel(\"Average Rating (vote_average)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"plot_01_line_avg_rating_by_year.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# 2) Area Plot — Total revenue over time (shows trend + magnitude)\n",
    "yearly_rev = viz_df.groupby(\"release_year\")[\"revenue\"].sum().sort_index()\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.fill_between(yearly_rev.index, yearly_rev.values, alpha=0.4)\n",
    "plt.plot(yearly_rev.index, yearly_rev.values, linewidth=1.5)\n",
    "plt.title(\"Area Plot: Total Reported Revenue by Release Year\")\n",
    "plt.xlabel(\"Release Year\")\n",
    "plt.ylabel(\"Total Revenue\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"plot_02_area_total_revenue_by_year.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# 3) Histogram — Runtime distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(viz_df[\"runtime\"], bins=30, edgecolor=\"black\", alpha=0.8)\n",
    "plt.title(\"Histogram: Runtime Distribution\")\n",
    "plt.xlabel(\"Runtime (minutes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"plot_03_hist_runtime.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# 4) Bar Chart — Top genres by movie count\n",
    "top_genres = viz_df[\"genre_main\"].value_counts().head(10)\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(x=top_genres.index, y=top_genres.values)\n",
    "plt.title(\"Bar Chart: Top 10 Main Genres by Count\")\n",
    "plt.xlabel(\"Main Genre\")\n",
    "plt.ylabel(\"Number of Movies\")\n",
    "plt.xticks(rotation=35, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"plot_04_bar_top_genres.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# 5) Pie Chart — Language distribution (top 5 + Other)\n",
    "lang_counts = viz_df[\"original_language\"].value_counts()\n",
    "top5 = lang_counts.head(5)\n",
    "other = pd.Series({\"other\": lang_counts.iloc[5:].sum()})\n",
    "pie_data = pd.concat([top5, other])\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie(pie_data.values, labels=pie_data.index, autopct=\"%1.1f%%\", startangle=90)\n",
    "plt.title(\"Pie Chart: Original Language Distribution (Top 5 + Other)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"plot_05_pie_language.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# 6) Box Plot — Rating by genre (top 6 genres)\n",
    "top6_genres = viz_df[\"genre_main\"].value_counts().head(6).index\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(data=viz_df[viz_df[\"genre_main\"].isin(top6_genres)], x=\"genre_main\", y=\"vote_average\")\n",
    "plt.title(\"Box Plot: Rating Distribution by Main Genre (Top 6)\")\n",
    "plt.xlabel(\"Main Genre\")\n",
    "plt.ylabel(\"Rating (vote_average)\")\n",
    "plt.xticks(rotation=35, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"plot_06_box_rating_by_genre.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# 7) Scatter Plot — Popularity vs rating\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(viz_df[\"popularity\"], viz_df[\"vote_average\"], alpha=0.25)\n",
    "plt.title(\"Scatter Plot: Popularity vs Rating\")\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Rating (vote_average)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"plot_07_scatter_popularity_vs_rating.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# 8) Bubble Plot — Popularity vs revenue (size = vote_count)\n",
    "bubble = viz_df.copy()\n",
    "bubble[\"vote_count_clip\"] = bubble[\"vote_count\"].clip(upper=bubble[\"vote_count\"].quantile(0.95))\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.scatter(\n",
    "    bubble[\"popularity\"],\n",
    "    bubble[\"revenue\"],\n",
    "    s=(bubble[\"vote_count_clip\"] / bubble[\"vote_count_clip\"].max()) * 300 + 10,\n",
    "    alpha=0.25\n",
    " )\n",
    "plt.title(\"Bubble Plot: Popularity vs Revenue (Bubble Size = Vote Count)\")\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"plot_08_bubble_popularity_vs_revenue.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfef0e0",
   "metadata": {},
   "source": [
    "## Interpretation of Visualizations (Academic Summary)\n",
    "- **Ratings over time (line):** highlights temporal drift in average ratings that may be linked to sampling bias, voting behavior, or catalog coverage.\n",
    "- **Revenue over time (area):** demonstrates strong concentration and non-stationarity; motivates log-transform for regression.\n",
    "- **Runtime histogram:** shows typical runtimes and a long-tail, indicating potential outliers or special formats.\n",
    "- **Top genres (bar):** confirms that the dataset is not genre-balanced; segmentation is necessary.\n",
    "- **Language pie:** indicates dominance of a few languages, which can influence global revenue and rating patterns.\n",
    "- **Genre box plots:** reveals systematic genre differences in rating distributions.\n",
    "- **Popularity vs rating (scatter):** popularity is not a perfect proxy for quality; both should be modeled separately.\n",
    "- **Popularity vs revenue (bubble):** revenue relates to popularity but with large variance; vote_count adds additional information as an engagement signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dba336",
   "metadata": {},
   "source": [
    "# Part 6 — Machine Learning Models\n",
    "## 6.1 Tasks\n",
    "- **Classification task:** Predict `is_high_rated` from metadata features.\n",
    "- **Regression task:** Predict `log_revenue` for movies with `revenue > 0`.\n",
    "\n",
    "## 6.2 Inputs / Outputs\n",
    "**Inputs (features):** numeric + categorical features derived from metadata (`popularity`, `vote_count`, `runtime`, `release_year`, `genre_main`, `original_language`, text lengths).\n",
    "\n",
    "**Outputs (targets):**\n",
    "- Classification: binary label `is_high_rated`\n",
    "- Regression: continuous target `log_revenue`\n",
    "\n",
    "## 6.3 Encoding\n",
    "We use a **ColumnTransformer** for preprocessing. For categorical features we use OneHot encoding (a robust form of categorical encoding). We also demonstrate Label Encoding for the target label, which is standard for binary classification targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Classification: define target, preprocessing, models, evaluation\n",
    "ml_df = df_model_base.copy()\n",
    "\n",
    "# Classification target\n",
    "RATING_THRESHOLD = 7.5\n",
    "ml_df[\"is_high_rated\"] = (ml_df[\"vote_average\"] >= RATING_THRESHOLD).astype(int)\n",
    "print(\"Class balance (is_high_rated):\")\n",
    "display(ml_df[\"is_high_rated\"].value_counts(normalize=True))\n",
    "\n",
    "# Features (keep simple + reproducible)\n",
    "feature_cols_num = [c for c in [\"popularity\",\"vote_count\",\"runtime\",\"release_year\",\"overview_len\",\"tagline_len\",\"genre_count\"] if c in ml_df.columns]\n",
    "feature_cols_cat = [c for c in [\"original_language\",\"genre_main\"] if c in ml_df.columns]\n",
    "feature_cols = feature_cols_num + feature_cols_cat\n",
    "\n",
    "X = ml_df[feature_cols].copy()\n",
    "y = ml_df[\"is_high_rated\"].copy()\n",
    "\n",
    "# Label encoding (demonstration for target; y is already 0/1 but included to meet requirement explicitly)\n",
    "le = LabelEncoder()\n",
    "y_le = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_le, test_size=0.2, random_state=RANDOM_STATE, stratify=y_le\n",
    " )\n",
    "\n",
    "def make_preprocessor(scale_mode: str):\n",
    "    \"\"\"scale_mode in {'none','standard','minmax'}\"\"\"\n",
    "    if scale_mode == \"standard\":\n",
    "        scaler_step = (\"scaler\", StandardScaler())\n",
    "    elif scale_mode == \"minmax\":\n",
    "        scaler_step = (\"scaler\", MinMaxScaler())\n",
    "    else:\n",
    "        scaler_step = None\n",
    "\n",
    "    num_steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "    if scaler_step is not None:\n",
    "        num_steps.append(scaler_step)\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=num_steps)\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, feature_cols_num),\n",
    "            (\"cat\", categorical_transformer, feature_cols_cat),\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "clf_models = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=2000, n_jobs=None),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE),\n",
    "    \"SVC_RBF\": SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_STATE),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=15)\n",
    "}\n",
    "\n",
    "def evaluate_classification(scale_mode: str):\n",
    "    pre = make_preprocessor(scale_mode)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    results = []\n",
    "    for name, model in clf_models.items():\n",
    "        pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "        scoring = {\n",
    "            \"accuracy\": \"accuracy\",\n",
    "            \"precision\": \"precision\",\n",
    "            \"recall\": \"recall\",\n",
    "            \"roc_auc\": \"roc_auc\",\n",
    "        }\n",
    "        cv_out = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=None)\n",
    "        results.append({\n",
    "            \"scale_mode\": scale_mode,\n",
    "            \"model\": name,\n",
    "            \"cv_accuracy\": np.mean(cv_out[\"test_accuracy\"]),\n",
    "            \"cv_precision\": np.mean(cv_out[\"test_precision\"]),\n",
    "            \"cv_recall\": np.mean(cv_out[\"test_recall\"]),\n",
    "            \"cv_roc_auc\": np.mean(cv_out[\"test_roc_auc\"]),\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "cls_results = pd.concat([\n",
    "    evaluate_classification(\"none\"),\n",
    "    evaluate_classification(\"standard\"),\n",
    "    evaluate_classification(\"minmax\"),\n",
    "], ignore_index=True)\n",
    "display(cls_results.sort_values([\"scale_mode\",\"cv_roc_auc\"], ascending=[True, False]))\n",
    "\n",
    "# Visualize comparisons (ROC-AUC)\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(data=cls_results, x=\"model\", y=\"cv_roc_auc\", hue=\"scale_mode\")\n",
    "plt.title(\"Classification Model Comparison (5-fold CV ROC-AUC)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"CV ROC-AUC\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_classification_cv_rocauc_comparison.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b1352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 Classification: finalize best model + test metrics + ROC curve\n",
    "best_row = cls_results.sort_values(\"cv_roc_auc\", ascending=False).iloc[0]\n",
    "best_scale = best_row[\"scale_mode\"]\n",
    "best_model_name = best_row[\"model\"]\n",
    "print(\"Best (by CV ROC-AUC):\", dict(best_row))\n",
    "\n",
    "best_pre = make_preprocessor(best_scale)\n",
    "best_model = clf_models[best_model_name]\n",
    "best_pipe = Pipeline(steps=[(\"pre\", best_pre), (\"model\", best_model)])\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "y_proba = best_pipe.predict_proba(X_test)[:, 1] if hasattr(best_pipe.named_steps[\"model\"], \"predict_proba\") else None\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "print(f\"Test Precision: {prec:.3f}\")\n",
    "print(f\"Test Recall: {rec:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_classification_confusion_matrix.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "if y_proba is not None:\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Test Set)\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"ml_classification_roc_curve.png\", dpi=160)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9026cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6 Regression: predict log-revenue for movies with revenue > 0\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "reg_df = df_model_base.copy()\n",
    "reg_df = reg_df[reg_df[\"revenue\"] > 0].copy()\n",
    "reg_df[\"log_revenue\"] = np.log1p(reg_df[\"revenue\"])\n",
    "print(\"Regression dataset shape:\", reg_df.shape)\n",
    "\n",
    "Xr = reg_df[feature_cols].copy()\n",
    "yr = reg_df[\"log_revenue\"].copy()\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    Xr, yr, test_size=0.2, random_state=RANDOM_STATE\n",
    " )\n",
    "\n",
    "reg_models = {\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=400, random_state=RANDOM_STATE),\n",
    "    \"SVR_RBF\": SVR(kernel=\"rbf\"),\n",
    "    \"KNN\": KNeighborsRegressor(n_neighbors=25)\n",
    "}\n",
    "\n",
    "def evaluate_regression(scale_mode: str):\n",
    "    pre = make_preprocessor(scale_mode)\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    scoring = {\n",
    "        \"mae\": \"neg_mean_absolute_error\",\n",
    "        \"mse\": \"neg_mean_squared_error\",\n",
    "        \"medae\": make_scorer(median_absolute_error, greater_is_better=False),\n",
    "        \"r2\": \"r2\",\n",
    "    }\n",
    "    out_rows = []\n",
    "    for name, model in reg_models.items():\n",
    "        pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "        cv_out = cross_validate(pipe, Xr_train, yr_train, cv=cv, scoring=scoring, n_jobs=None)\n",
    "        out_rows.append({\n",
    "            \"scale_mode\": scale_mode,\n",
    "            \"model\": name,\n",
    "            \"cv_MAE\": -np.mean(cv_out[\"test_mae\"]),\n",
    "            \"cv_MSE\": -np.mean(cv_out[\"test_mse\"]),\n",
    "            \"cv_MedAE\": -np.mean(cv_out[\"test_medae\"]),\n",
    "            \"cv_R2\": np.mean(cv_out[\"test_r2\"]),\n",
    "        })\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "reg_results = pd.concat([\n",
    "    evaluate_regression(\"none\"),\n",
    "    evaluate_regression(\"standard\"),\n",
    "    evaluate_regression(\"minmax\"),\n",
    "], ignore_index=True)\n",
    "display(reg_results.sort_values([\"scale_mode\",\"cv_R2\"], ascending=[True, False]))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.barplot(data=reg_results, x=\"model\", y=\"cv_R2\", hue=\"scale_mode\")\n",
    "plt.title(\"Regression Model Comparison (5-fold CV $R^2$ on log-revenue)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"CV $R^2$\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_regression_cv_r2_comparison.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7 Regression: finalize best model + test metrics + diagnostic plots\n",
    "best_reg_row = reg_results.sort_values(\"cv_R2\", ascending=False).iloc[0]\n",
    "best_reg_scale = best_reg_row[\"scale_mode\"]\n",
    "best_reg_model_name = best_reg_row[\"model\"]\n",
    "print(\"Best regression (by CV R2):\", dict(best_reg_row))\n",
    "\n",
    "best_reg_pre = make_preprocessor(best_reg_scale)\n",
    "best_reg_model = reg_models[best_reg_model_name]\n",
    "best_reg_pipe = Pipeline(steps=[(\"pre\", best_reg_pre), (\"model\", best_reg_model)])\n",
    "best_reg_pipe.fit(Xr_train, yr_train)\n",
    "\n",
    "yr_pred = best_reg_pipe.predict(Xr_test)\n",
    "mae = mean_absolute_error(yr_test, yr_pred)\n",
    "mse = mean_squared_error(yr_test, yr_pred)\n",
    "medae = median_absolute_error(yr_test, yr_pred)\n",
    "r2 = r2_score(yr_test, yr_pred)\n",
    "print(f\"Test MAE: {mae:.3f}\")\n",
    "print(f\"Test MSE: {mse:.3f}\")\n",
    "print(f\"Test Median AE: {medae:.3f}\")\n",
    "print(f\"Test R2: {r2:.3f}\")\n",
    "\n",
    "# Predicted vs Actual\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(yr_test, yr_pred, alpha=0.3)\n",
    "lims = [min(yr_test.min(), yr_pred.min()), max(yr_test.max(), yr_pred.max())]\n",
    "plt.plot(lims, lims, \"--\", color=\"gray\")\n",
    "plt.title(\"Regression: Predicted vs Actual (log-revenue)\")\n",
    "plt.xlabel(\"Actual log(1+revenue)\")\n",
    "plt.ylabel(\"Predicted log(1+revenue)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_regression_pred_vs_actual.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# Residuals\n",
    "residuals = yr_test - yr_pred\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(yr_pred, residuals, alpha=0.3)\n",
    "plt.axhline(0, linestyle=\"--\", color=\"gray\")\n",
    "plt.title(\"Regression Residuals vs Predicted\")\n",
    "plt.xlabel(\"Predicted log(1+revenue)\")\n",
    "plt.ylabel(\"Residual (actual - predicted)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_regression_residuals.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752089b",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- The dataset exhibits real-world messiness (missing marketing text, zero revenues, skewed distributions, extreme outliers).\n",
    "- Genre/language/decade segmentation reveals systematic differences in typical ratings and revenue behavior.\n",
    "- Model comparisons show how **scaling** (StandardScaler) and **normalization** (MinMax) can materially affect distance-based models (KNN, SVR/SVC), while tree-based models are typically less sensitive.\n",
    "- The final deliverables (PDF + ZIP) are generated programmatically for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9d64e",
   "metadata": {},
   "source": [
    "# Deliverables\n",
    "This section generates the required submission artifacts:\n",
    "- **PDF report** with cover page, problem domain, summary, (linked) source code, and visualization snapshots.\n",
    "- **ZIP package** containing the notebook, dataset (or download script), and exported figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Generate FINAL REPORT (PDF)\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "REPORT_PATH = PROJECT_DIR / \"Final_Report.pdf\"\n",
    "\n",
    "TEAM_MEMBERS = [\n",
    "    (\"Mohamed Mostafa\", \"23101594\"),\n",
    "    (\"Marwan Khaled\", \"23101599\"),\n",
    "    (\"Mohamed Adel\", \"23101899\"),\n",
    "]\n",
    "\n",
    "def add_text_page(pdf: PdfPages, title: str, lines: list[str]):\n",
    "    fig = plt.figure(figsize=(8.27, 11.69))  # A4 portrait\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    plt.axis(\"off\")\n",
    "    y = 0.95\n",
    "    plt.text(0.07, y, title, fontsize=18, fontweight=\"bold\", va=\"top\")\n",
    "    y -= 0.05\n",
    "    wrapped = []\n",
    "    for line in lines:\n",
    "        wrapped.extend(textwrap.wrap(str(line), width=95) or [\"\"])\n",
    "    for line in wrapped:\n",
    "        plt.text(0.07, y, line, fontsize=11, va=\"top\")\n",
    "        y -= 0.02\n",
    "        if y < 0.08:\n",
    "            break\n",
    "    pdf.savefig(fig, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def add_image_page(pdf: PdfPages, title: str, image_path: Path):\n",
    "    fig = plt.figure(figsize=(8.27, 11.69))\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(0.07, 0.95, title, fontsize=16, fontweight=\"bold\", va=\"top\")\n",
    "    if image_path.exists():\n",
    "        img = plt.imread(image_path)\n",
    "        ax = fig.add_axes([0.07, 0.10, 0.86, 0.80])\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "    else:\n",
    "        plt.text(0.07, 0.85, f\"Missing figure: {image_path.name}\", fontsize=12)\n",
    "    pdf.savefig(fig, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_table_figure(df_table: pd.DataFrame, title: str, out_path: Path) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(11.0, 4.2))\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(title, fontsize=14, fontweight=\"bold\", pad=12)\n",
    "    display_df = df_table.copy()\n",
    "    display_df.index.name = \"Model\"\n",
    "    display_df = display_df.reset_index()\n",
    "    tbl = ax.table(\n",
    "        cellText=display_df.values,\n",
    "        colLabels=display_df.columns,\n",
    "        cellLoc=\"center\",\n",
    "        loc=\"center\",\n",
    "    )\n",
    "    tbl.auto_set_font_size(False)\n",
    "    tbl.set_fontsize(10)\n",
    "    tbl.scale(1.0, 1.4)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "cover_lines = [\n",
    "    \"Course Title: Machine Learning\",\n",
    "    \"Team Members:\",\n",
    "    *[f\"- {name} : {sid}\" for name, sid in TEAM_MEMBERS],\n",
    "    \"\",\n",
    "    f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n",
    "    \"\",\n",
    "    \"Project: Movie Metadata Analytics for Streaming Acquisition Decisions\",\n",
    "]\n",
    "\n",
    "summary_lines = [\n",
    "    \"Problem Domain: Streaming platform decision support using real-world movie metadata.\",\n",
    "    f\"Dataset: Top_10000_Movies.csv ({df_raw.shape[0]} rows, {df_raw.shape[1]} columns).\",\n",
    "    \"Dirty data indicators: missing/blank text, zero revenues, skewed distributions, outliers, and semi-structured categorical genres.\",\n",
    "    \"ML tasks: (1) Classification of High Rated movies; (2) Regression for log-revenue.\",\n",
    "    \"Models: Logistic Regression, Random Forest, SVC, KNN (classification) and Linear/Ridge, Random Forest Regressor, SVR, KNN (regression).\",\n",
    "    \"Evaluation: Cross-validation and test metrics; scaling vs normalization comparisons included.\",\n",
    "]\n",
    "\n",
    "code_lines = [\n",
    "    \"Source Code: This submission includes Ml_Project.ipynb and a template download script.\",\n",
    "    \"Preprocessing uses ColumnTransformer with robust imputation and one-hot encoding.\",\n",
    "    \"Models are compared under three preprocessing modes: none, StandardScaler (scaling), and MinMax (normalization).\",\n",
    "]\n",
    "\n",
    "# Build model-comparison tables (required: compare the 4 models)\n",
    "cls_table = (\n",
    "    cls_results.pivot(index=\"model\", columns=\"scale_mode\", values=\"cv_roc_auc\")\n",
    "    .reindex(columns=[c for c in [\"none\", \"standard\", \"minmax\"] if c in cls_results[\"scale_mode\"].unique()])\n",
    "    .round(3)\n",
    "    .sort_index()\n",
    " )\n",
    "reg_table = (\n",
    "    reg_results.pivot(index=\"model\", columns=\"scale_mode\", values=\"cv_R2\")\n",
    "    .reindex(columns=[c for c in [\"none\", \"standard\", \"minmax\"] if c in reg_results[\"scale_mode\"].unique()])\n",
    "    .round(3)\n",
    "    .sort_index()\n",
    " )\n",
    "save_table_figure(cls_table, \"Classification (4 Models): Mean CV ROC-AUC by Preprocessing Mode\", FIG_DIR / \"ml_classification_comparison_table.png\")\n",
    "save_table_figure(reg_table, \"Regression (4 Models): Mean CV R² by Preprocessing Mode (log-revenue)\", FIG_DIR / \"ml_regression_comparison_table.png\")\n",
    "\n",
    "# Optional dataset download script (reproducibility template)\n",
    "DOWNLOAD_SCRIPT = PROJECT_DIR / \"download_dataset.py\"\n",
    "download_script_content = '''#!/usr/bin/env python\n",
    "\"\"\"Recreate the dataset via TMDb-style metadata extraction (requires an API key).\n",
    "\n",
    "This script is provided to satisfy the deliverable requirement of including a\n",
    "dataset download/recreation option. If you already have Top_10000_Movies.csv,\n",
    "you do NOT need to run this.\n",
    "\n",
    "Note: Actual API access requires registering for an API key with the chosen provider\n",
    "and implementing provider-specific requests + pagination.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT = Path.cwd() / \"Top_10000_Movies.csv\"\n",
    "API_KEY = os.getenv(\"TMDB_API_KEY\", \"\")\n",
    "\n",
    "def main():\n",
    "    if not API_KEY:\n",
    "        raise SystemExit(\"Set TMDB_API_KEY env var before running.\")\n",
    "    raise SystemExit(\"Template script: implement API calls if required by your course.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "DOWNLOAD_SCRIPT.write_text(download_script_content, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", DOWNLOAD_SCRIPT)\n",
    "\n",
    "# Build the PDF report\n",
    "figure_pages = [\n",
    "    (\"Model Comparison (Classification Table)\", FIG_DIR / \"ml_classification_comparison_table.png\"),\n",
    "    (\"Model Comparison (Regression Table)\", FIG_DIR / \"ml_regression_comparison_table.png\"),\n",
    "    (\"EDA: Correlation Heatmap\", FIG_DIR / \"eda_corr_heatmap.png\"),\n",
    "    (\"Visualization 1: Line Plot\", FIG_DIR / \"plot_01_line_avg_rating_by_year.png\"),\n",
    "    (\"Visualization 2: Area Plot\", FIG_DIR / \"plot_02_area_total_revenue_by_year.png\"),\n",
    "    (\"Visualization 3: Histogram\", FIG_DIR / \"plot_03_hist_runtime.png\"),\n",
    "    (\"Visualization 4: Bar Chart\", FIG_DIR / \"plot_04_bar_top_genres.png\"),\n",
    "    (\"Visualization 5: Pie Chart\", FIG_DIR / \"plot_05_pie_language.png\"),\n",
    "    (\"Visualization 6: Box Plot\", FIG_DIR / \"plot_06_box_rating_by_genre.png\"),\n",
    "    (\"Visualization 7: Scatter Plot\", FIG_DIR / \"plot_07_scatter_popularity_vs_rating.png\"),\n",
    "    (\"Visualization 8: Bubble Plot\", FIG_DIR / \"plot_08_bubble_popularity_vs_revenue.png\"),\n",
    "    (\"Classification: CV ROC-AUC Comparison\", FIG_DIR / \"ml_classification_cv_rocauc_comparison.png\"),\n",
    "    (\"Classification: Confusion Matrix\", FIG_DIR / \"ml_classification_confusion_matrix.png\"),\n",
    "    (\"Classification: ROC Curve\", FIG_DIR / \"ml_classification_roc_curve.png\"),\n",
    "    (\"Regression: CV R2 Comparison\", FIG_DIR / \"ml_regression_cv_r2_comparison.png\"),\n",
    "    (\"Regression: Predicted vs Actual\", FIG_DIR / \"ml_regression_pred_vs_actual.png\"),\n",
    "    (\"Regression: Residuals\", FIG_DIR / \"ml_regression_residuals.png\"),\n",
    "]\n",
    "\n",
    "with PdfPages(REPORT_PATH) as pdf:\n",
    "    add_text_page(pdf, \"Cover Page\", cover_lines)\n",
    "    add_text_page(pdf, \"Section 1: Problem Domain\", [\n",
    "        \"We model movie performance signals to support streaming acquisition decisions.\",\n",
    "        \"Classification target: is_high_rated derived from vote_average.\",\n",
    "        \"Regression target: log(1+revenue) for movies with reported revenue.\",\n",
    "    ])\n",
    "    add_text_page(pdf, \"Section 2: Project Summary\", summary_lines)\n",
    "    add_text_page(pdf, \"Section 3: Source Code\", code_lines)\n",
    "    add_text_page(pdf, \"Section 4: Visualization Snapshots\", [\n",
    "        \"The following pages include snapshots of model comparisons, EDA, required visualizations, and model evaluation plots.\",\n",
    "    ])\n",
    "    for title, path in figure_pages:\n",
    "        add_image_page(pdf, title, path)\n",
    "\n",
    "print(\"Generated PDF report:\", REPORT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b947424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Create ZIP package (notebook + dataset + figures + PDF + script)\n",
    "ZIP_PATH = PROJECT_DIR / \"Submission_Package.zip\"\n",
    "\n",
    "files_to_include = [\n",
    "    PROJECT_DIR / \"Ml_Project.ipynb\",\n",
    "    DATA_PATH,\n",
    "    REPORT_PATH,\n",
    "    DOWNLOAD_SCRIPT,\n",
    "]\n",
    "\n",
    "# Add all figures\n",
    "figure_files = sorted(FIG_DIR.glob(\"*.png\"))\n",
    "files_to_include.extend(figure_files)\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for p in files_to_include:\n",
    "        if p.exists():\n",
    "            # Place figures under figures/ inside the zip\n",
    "            arcname = p.name\n",
    "            if p.parent.name == \"figures\":\n",
    "                arcname = f\"figures/{p.name}\"\n",
    "            zf.write(p, arcname=arcname)\n",
    "        else:\n",
    "            print(\"Missing (skipped):\", p)\n",
    "\n",
    "print(\"Created ZIP:\", ZIP_PATH)\n",
    "print(\"Included figures:\", len(figure_files))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
